%!TeX spellcheck = en_GB
% Die erste (unkommentierte) Zeile im Dokument legt immer die
% Dokumentklasse fest
\documentclass{scrartcl} 

% Präambel:
% Einbinen von zusätzlichen Paketen. Falls für eine Datei keine Endung
% explizit angegeben wird, benutzt LaTeX '.tex'. Im Folgenden wird
% also die Datei 'edv_pakete.tex' eingebunden.
\input{edv_pakete}


% Verzeichnisse mit Abbildungen; kann gestrichen werden,
% falls Sie dies schon in edv_pakete.tex definiert haben:
%\graphicspath{{../report}}

\addbibresource{refs.bib} %Hinzufügen einer Literaturdatenbank aus dem angegebenen Verzeichnis

% Titel, Autor und Datum
\title{Computational Physics}
\subtitle{Exercise 3}
\date{\today}
\author{Christiane Groß, Nico Dichter}

% Jetzt startet das eigentliche Dokument
\begin{document}
	\maketitle
\section{The long range Ising model}
We want to look at the long range Ising model, where an interaction with all other lattice points is considered. The theory was given in the lectures and on the exercise sheet. Inserting $\hat{J}=J/N$ to get normalizable solutions, we have: 
\begin{equation}
H_I(s)=-\frac{1}{2}\hat{J}\sum_{i,j}s_is_j-h\sum_{i,j}s_i
\label{eq:hamiltonianising}
\end{equation}

To be able to use HMC, we need to work in continuous space and the derivations on the sheet give us for $J>0$, using the Hubbard-Stratonovich-transformation:

\begin{equation}
Z=\sum_{\{s_i=\pm1\}}\exp(-\beta H_I(s,h))=
\int_{-\infty}^{\infty}\frac{\mathrm{d} \phi}{\sqrt{2\pi\beta\hat{J}}}
\exp\left( -\frac{\phi^2}{2\beta\hat{J}}+N\log\left( 2\cosh(\beta h\pm\phi)\right) \right) 
\label{eq:partfunc}
\end{equation}


We always choose $+\phi$ in the argument of the $\cosh$ and introduce the effective action:
\begin{equation}
S(\phi)=\frac{\phi^2}{2\beta\hat{J}}-N\log\left( 2\cosh(\beta h+\phi)\right)
\end{equation}

and the effective Hamiltonian:
\begin{equation}
H(\phi, p)=\frac{p^2}{2}+S(\phi)
\end{equation}

\section{Deliberations}

\subsection{Observables}
We need to transform our observables into continuous functions of $\phi$.

\[\begin{array}{>{\displaystyle}r>{\displaystyle}c>{\displaystyle}l}

Z&=&\int_{-\infty}^{\infty}\frac{\mathrm{d} \phi}{\sqrt{2\pi\beta\hat{J}}}exp(-S(\phi))\\

%\langle O\rangle&=&\frac{1}{Z}\int_{-\infty}^{\infty}\frac{\mathrm{d} \phi}{\sqrt{2\pi\beta\hat{J}}}O(\phi)\exp(-S(\phi))\\

\dpd{\log(Z)}{x}&=&\frac{1}{Z}\dpd{Z}{x}=
-\frac{1}{Z}\int_{-\infty}^{\infty}\frac{\mathrm{d} \phi}{\sqrt{2\pi\beta\hat{J}}}\exp(-S(\phi))%\dpd{S(\phi)}{x}\\
\left( \dpd{S(\phi)}{x}-\sqrt{2\pi\beta\hat{J}}\dpd{}{x}\frac{1}{\sqrt{2\pi\beta\hat{J}}}\right) \\

\langle m\rangle&=&\frac{1}{Z}\int_{-\infty}^{\infty}\frac{\mathrm{d} \phi}{\sqrt{2\pi\beta\hat{J}}}m(\phi)\exp(-S(\phi))\\
&=&\frac{1}{N\beta}\dpd{\log(Z)}{h}=
-\frac{1}{NZ\beta}\int_{-\infty}^{\infty}\frac{\mathrm{d} \phi}{\sqrt{2\pi\beta\hat{J}}}\dpd{S(\phi)}{h}\exp(-S(\phi))\\

\Rightarrow m(\phi)&=&-\frac{1}{N\beta}\dpd{S(\phi)}{h}\\
&=&(-)^2\frac{1}{N\beta}\frac{2N\beta\sinh(\beta h+\phi)}{2\cosh(\beta h+\phi)}\\
&=&\tanh(\beta h+\phi)\\


\end{array}\]
\[\begin{array}{>{\displaystyle}r>{\displaystyle}c>{\displaystyle}l}

\text{analoguously } \langle \epsilon\rangle&=&-\frac{1}{N}\dpd{\log(Z)}{\beta}=
(-)^2\frac{1}{NZ}\int_{-\infty}^{\infty}\frac{\mathrm{d} \phi}{\sqrt{2\pi\beta\hat{J}}}\exp(-S(\phi))%\dpd{S(\phi)}{\beta}\\
\left( \dpd{S(\phi)}{\beta}-\sqrt{2\pi\beta\hat{J}}\dpd{}{\beta}\frac{1}{\sqrt{2\pi\beta\hat{J}}}\right)\\

\Rightarrow \epsilon(\phi)&=&\frac{1}{N}\left( \dpd{S(\phi)}{\beta}-\sqrt{2\pi\beta\hat{J}}\dpd{}{\beta}\frac{1}{\sqrt{2\pi\beta\hat{J}}}\right)\\
%\Rightarrow \epsilon(\phi)&=&\frac{1}{N} \dpd{S(\phi)}{\beta}\\
&=&-\frac{\phi^2}{2\beta^2 N\hat{J}}-\frac{2Nh\sinh(\beta h+\phi)}{N2\cosh(\beta h+\phi)}+\frac{2\pi\hat{J}}{2N\cdot2\pi\hat{J}\beta}\\

&=&-h\tanh(\beta h+\phi)-\frac{\phi^2}{2\beta^2 N\hat{J}}+\frac{1}{2N\beta}\\

\Rightarrow \beta\epsilon(\phi)&=&-\beta h\tanh(\beta h+\phi)-\frac{\phi^2}{2\beta N\hat{J}}+\frac{1}{2N}\\

\end{array}\]

We can insert these definitions into our code. We will not calculate $\langle \epsilon\rangle$ but instead $\langle \beta\epsilon\rangle$, because this allows us to absorb $\beta$ into $\hat{J}$ and $h$, and is also the form of the analytical solutions given on the sheet.



\subsection{Equations of Motion}

To be able to use HMC, we need to evolve $p$ and $\phi$ along a trajectory along which $H$ is approximately constant. We do this by integrating over the equations of motion:

\[\begin{array}{>{\displaystyle}r>{\displaystyle}c>{\displaystyle}l}
\dot{p}&=&-\dpd{H}{\phi}\\
&=&\frac{\phi}{\beta \hat{J}}-N\tanh(\beta h+\phi)\\
\dot{\phi}&=&\dpd{H}{p}=p
\end{array}\]

\subsection{HMC}

\subsection{Leapfrog}

\subsection{Error estimation}
When determining errors for our measurements, we use binning and bootstrapping instead of the naive standard deviation. 
We do this as described in the lecture: First, we bin our data $O_i$ from $N$ measurements in $\lfloor N/l\rfloor$ bins $O^B_i=\frac{1}{l}\sum_{k=1}^l O_{(i-1)\cdot l+k}$. 

From these new datapoints, we make $R$ bootstrap replicas. For each replica, we sample with replacement from the given $O_i^B$, and take the arithmetic mean over those chosen values to be the replica. At the end, we can estimate the mean and error of our measurements as the arithmetic mean and standard deviation over $R$ bootstrap replicas.

This ensures we minimize the effects of correlation on our data and also get a realisitic error estimate.
%Binning to reduce correlation
%Bootstrap to estimate error

\section{Simulation}
In the implementation of the simulation gsl\_vector objects  and views on these are used to accomplish proper binning (see \cite{gsldoc_mat}). 

At first the convergence of the leap frog algorithm gets tested and the result is shown in fig. \ref{fig:converge}. Then for each parameter set (N,J) ($h=0.5$ fixed) a large amount of thermalization steps are made and afterwards several measurements of the wanted physical variable take place. A step in this case means to sample a conjugated momentum from the normal distribution and performing the leap frog algorithm with this momentum. Afterwards the relative change in total energy is calculated and an accept/reject takes place with the acceptance probability of:
\begin{equation}
	P_{acc.}=\min(1,\exp(\cal{H} (\texttt{q},\phi)-\cal H (\texttt{q}',\phi')))
\end{equation} 
It has to be noted, that in case of rejection the configuration $\phi$ gets added again to the list of generated configurations.
The thermalization steps are needed to make sure the measurements take place on configurations which resemble the given parameter set. The generation of the initial set of observables take place after each step. To lower the correlation binning was used on top of this initial set. At the end bootstrap was used to estimate an appropriate error on top of the binned data.

\subsection{choosing bin length}

\section{Results}

\subsection{Convergence of leapfrog}

The convergence of our leapfrog algorithm is shown in fig.~\ref{fig:converge}. Qualitatively, it behaves the same as the expected one on the sheet: We start with a high value of $\Delta$, and it gets smaller with each step we add to the algorithm. 

Depending on our starting parameters, $\Delta$ is sheifted by about one order of magneitude, which also explains why our values are different from the ones on the sheet.
We see that our leapfrog algorithm converges quite nicely and so we can use it for the rest of our calculations.
\begin{figure}[htbp]
	\input{converge_t.tex}
	\caption{convergence of the leapfrog algorithm}
	\label{fig:converge}
\end{figure}

Our acceptance rates are quite high for large $N_{md}$, so we choose $N_{md}=4$ to get an acceptance rate of about 70\%. This is in the range of acceptance rates we aim for.
%plot description tuning of acceptance rate
\subsection{Magnetization}

The analytical expectation and our measurements for the magnetization are plotted in fig.~\ref{fig:magnetization}. We expect the magnetization to be about $0.5$ for $J=0.2$, and then rise monotonuously. This rise gets less steep for growing $J$, and does not reach $m=1$. For smaller $N$, we expect the magnetization to be smaller and its rise to be less steep than for larger $N$, however the distance between the lines decreases for growing $N$.

Almost all of our measurements lie on the expected curves, there are some outliers for small $J$. 

The errors are quite small for all $N$, but for smaller $N$, but they rise for smaller $N$.
\begin{figure}[htbp]
	\input{magnetization_t.tex}
	\caption{magnetization for different $N$}
	\label{fig:magnetization}
\end{figure}
%plot description

\subsection{Average energy per site}

The expected values and our measurements for $\langle \beta\epsilon\rangle$ are plotted in fig.~\ref{fig:energy}. We expect $\langle \beta\epsilon\rangle\approx-0.3$ for $J=0.2$, and for the enrgy to decrease monotonuosly, with almost a constant descent. We also expect the energy levels of the different $N$ to grow further apart for growing $J$, and for the energy to be higher for smaller $N$.
\begin{figure}[htbp]
	\input{energy_t.tex}
	\caption{energy per site for different $N$}
	\label{fig:energy}
\end{figure}
plot description

\newpage	
\listoffigures
\printbibliography
\end{document}